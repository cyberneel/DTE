{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMBjvCXJ3wAszBdFrSAm89c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyberneel/DTE/blob/main/DTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design Text Extractor (DTE)\n",
        "\n",
        "**What is it:** This notebook is a projecct to get the specs for machines from their designs easier."
      ],
      "metadata": {
        "id": "jjuYG5pZ21QY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK-WvLKo2tfr"
      },
      "outputs": [],
      "source": [
        "import google\n",
        "\n",
        "# Installing ppytorch & its dependencies\n",
        "!pip install torch torchaudio torchvision torchtext torchdata\n",
        "google.colab.output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing EasyOCR (for text extracting)\n",
        "!pip install easyocr\n",
        "google.colab.output.clear()"
      ],
      "metadata": {
        "id": "J8fg-sQO3faa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependency Imports\n",
        "import cv2\n",
        "import easyocr\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-4uC_rC04Ycc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the iage path with this variable"
      ],
      "metadata": {
        "id": "WHt-2w7Y6Bfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = (\"/MachineDesignBox.jpeg\")"
      ],
      "metadata": {
        "id": "K9fKEgQG8iZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ocr = easyocr.Reader(['en'], gpu=True)"
      ],
      "metadata": {
        "id": "_rYs3Wr39KPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the basic detection\n",
        "result = ocr.readtext(IMAGE_PATH)\n",
        "result"
      ],
      "metadata": {
        "id": "s4lnx7j9-Dpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the texts\n",
        "\n",
        "import easyocr\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "image = cv2.imread(IMAGE_PATH)\n",
        "\n",
        "# Draw bounding boxes\n",
        "for (bbox, text, prob) in result:\n",
        "    (top_left, top_right, bottom_right, bottom_left) = bbox\n",
        "    top_left = tuple(map(int, top_left))\n",
        "    bottom_right = tuple(map(int, bottom_right))\n",
        "    cv2.rectangle(image, top_left, bottom_right, (0, 0, 0), 2)\n",
        "    cv2.putText(image, text, (top_left[0], top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "# Convertfrom BGR to RGB\n",
        "imageN = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image with the boundary boxes\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(imageN)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nt1G5U73A6MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grouping\n",
        "\n",
        "Here we try to group the text"
      ],
      "metadata": {
        "id": "nZnO61smIeg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# More dependencies\n",
        "!pip install scikit-learn opencv-python-headless\n",
        "google.colab.output.clear()"
      ],
      "metadata": {
        "id": "xBaPPeQJIiFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the scikit\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "image = cv2.imread(IMAGE_PATH)\n",
        "\n",
        "# Extract the centers of bounding boxes\n",
        "coordinates = []\n",
        "for (boundBox, text, prob) in result:\n",
        "    (top_left, top_right, bottom_right, bottom_left) = boundBox\n",
        "    center_x = (top_left[0] + bottom_right[0]) / 2\n",
        "    center_y = (top_left[1] + bottom_right[1]) / 2\n",
        "    coordinates.append([center_x, center_y])\n",
        "\n",
        "# Convert to numpy array\n",
        "coordinates = np.array(coordinates)\n",
        "\n",
        "# DBSCAN clustering\n",
        "clustering = DBSCAN(eps=100, min_samples=1).fit(coordinates)\n",
        "\n",
        "# Group the text by the clusters\n",
        "clustered_text = {}\n",
        "for label, (bbox, text, prob) in zip(clustering.labels_, result):\n",
        "    if label not in clustered_text:\n",
        "        clustered_text[label] = []\n",
        "    clustered_text[label].append((bbox, text, prob))\n"
      ],
      "metadata": {
        "id": "dkjqDGhEIs9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New preview of clusterd text\n",
        "\n",
        "# list to store the grouped text as objects\n",
        "grouped_text_objects = []\n",
        "\n",
        "# Draw bounding boxes of groups\n",
        "for cluster in clustered_text.values():\n",
        "    all_x = []\n",
        "    all_y = []\n",
        "    combined_text = ''\n",
        "    for (bbox, text, prob) in cluster:\n",
        "        (top_left, top_right, bottom_right, bottom_left) = bbox\n",
        "        top_left = tuple(map(int, top_left))\n",
        "        bottom_right = tuple(map(int, bottom_right))\n",
        "        #cv2.rectangle(image, top_left, bottom_right, (0, 0, 0), 2)\n",
        "        #cv2.putText(image, text, (top_left[0], top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "        all_x.extend([top_left[0], bottom_right[0]])\n",
        "        all_y.extend([top_left[1], bottom_right[1]])\n",
        "        combined_text += ' ' + text\n",
        "\n",
        "    # Store the combined text of the cluster\n",
        "    grouped_text_objects.append(combined_text.strip())\n",
        "\n",
        "    # Draw box around cluster\n",
        "    min_x, max_x = min(all_x), max(all_x)\n",
        "    min_y, max_y = min(all_y), max(all_y)\n",
        "    cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (0, 0, 255), 2)\n",
        "\n",
        "# Convert from BGR to RGB\n",
        "imageG = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(imageG)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(grouped_text_objects)"
      ],
      "metadata": {
        "id": "s6A2s472Kgio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Magic\n",
        "\n",
        "Try to use Natural Language Processing (NLP) to structure results"
      ],
      "metadata": {
        "id": "shSai-wxL5pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers safetensors huggingface_hub accelerate flash_attn\n",
        "google.colab.output.clear()"
      ],
      "metadata": {
        "id": "tObmIvpeLnOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "cq0grsLAVnUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"Here is the raw text, turn this into a json with all the appropriate fields & fix any typos: \" + grouped_text_objects[2], max_length=1000)\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "2h_27hDuVm7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Larger Samples\n",
        "\n",
        "Now lets try to make the tool detects the desired specs of machines from a larger diagram with multiple machines"
      ],
      "metadata": {
        "id": "_o1qKDnXaTOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking\n",
        "\n",
        "In this section we will first crop the design document to the regions we need"
      ],
      "metadata": {
        "id": "RigpGufId3wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# Assuming the uploaded image file is the first one\n",
        "image_path = '/IMG_2132.jpeg'#next(iter(uploaded))\n",
        "image = cv2.imread(image_path)\n",
        "hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "# Define the RGB color and convert to HSV\n",
        "target_color = np.uint8([[[242, 243, 245]]]) # Set to the color of the boxes needed\n",
        "hsv_target_color = cv2.cvtColor(target_color, cv2.COLOR_RGB2HSV)[0][0]\n",
        "\n",
        "# Define the HSV range for the target color\n",
        "lower_color = hsv_target_color - np.array([50, 20, 20]) # change these + & - for sensitivity\n",
        "upper_color = hsv_target_color + np.array([50, 20, 20])\n",
        "\n",
        "# Create a mask for the color\n",
        "mask = cv2.inRange(hsv_image, lower_color, upper_color)\n",
        "\n",
        "# Find contours in the mask\n",
        "contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Define the minimum contour area\n",
        "min_contour_area = 200000  # Adjust this value as needed\n",
        "\n",
        "# Iterate through the contours, filter by area, crop, and display the smaller images\n",
        "for idx, contour in enumerate(contours):\n",
        "    if cv2.contourArea(contour) >= min_contour_area:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        cropped_image = image[y:y+h, x:x+w]\n",
        "\n",
        "        # Convert BGR to RGB for displaying with matplotlib\n",
        "        cropped_image_rgb = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Display the cropped image\n",
        "        plt.figure()\n",
        "        plt.imshow(cropped_image_rgb)\n",
        "        plt.title(f'Cropped Image {idx+1}')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        print(cv2.contourArea(contour))\n",
        "\n",
        "print(\"Cropped images have been displayed.\")\n"
      ],
      "metadata": {
        "id": "-HMgnPo6hS8R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}